{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "printable-survivor",
   "metadata": {},
   "source": [
    "# EXTRACTIVE SUMMARIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-printing",
   "metadata": {},
   "source": [
    "#### Dependencies\n",
    "\n",
    "* To avoid dependency issues, install the following versions\n",
    "\n",
    "Python = 3.6.9 <br>\n",
    "torch==1.7.0 <br> \n",
    "spacy==2.3.1 <br>\n",
    "bert-extractive-summarizer <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "progressive-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "import traceback \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-registrar",
   "metadata": {},
   "source": [
    "### Parsing SRT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hearing-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtitle_to_textblob(subtitle_file):\n",
    "\n",
    "    input_text_list = list()\n",
    "    input_times_list = list()\n",
    "\n",
    "    count = 0\n",
    "    with open(subtitle_file, 'r') as fp:\n",
    "        input_lines = fp.readlines()\n",
    "        for line in input_lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            # print('Count ', count)\n",
    "            if (line):\n",
    "                # Process line numbers\n",
    "                if (count == 0):\n",
    "                    count += 1\n",
    "                elif (count == 1):\n",
    "                    input_times_list.append(line)\n",
    "                    count += 1\n",
    "                elif (count == 2):\n",
    "                    input_text_list.append(line)\n",
    "                    count = 0\n",
    "    return input_text_list, input_times_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "handled-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summarization(input_text, num_sentences, debug=False):\n",
    "\n",
    "    model = Summarizer()\n",
    "    output_text = model(input_text, num_sentences=num_sentences-1)\n",
    "    \n",
    "    if (debug):        \n",
    "        print('----------------- TOP',str(num_sentences),'SENTENCES -----------------')\n",
    "        print(output_text)\n",
    "        print('----------------------------------------------------')\n",
    "        \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "seasonal-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted_text_to_output(input_text, output_text, output_file, input_times_list, time_delimiter):\n",
    "    \n",
    "    try:\n",
    "        extracted_sentences = re.split(r'[.!?\\n]\\s*',output_text.strip())\n",
    "        # print('Size of times list: ', len(input_times_list))\n",
    "\n",
    "        with open(op_file, 'w') as fp:\n",
    "\n",
    "            for sentence in extracted_sentences:\n",
    "\n",
    "                sentence = sentence.strip()\n",
    "\n",
    "                if (sentence):\n",
    "\n",
    "                    # print(sentence)\n",
    "                    search_list = list(sentence.split())\n",
    "\n",
    "                    end_char_index = input_text.find(sentence)\n",
    "                    start_word_index = len(input_text[:end_char_index].split())\n",
    "                    end_word_index = start_word_index + len(search_list)-1\n",
    "                    \n",
    "                    # print(end_char_index, start_word_index, end_word_index)\n",
    "                    \n",
    "                    # print(start_word_index, end_word_index)\n",
    "\n",
    "                    start_ip_time = input_times_list[start_word_index].split(time_delimiter)[0].strip()\n",
    "                    # print(start_ip_time)\n",
    "\n",
    "                    end_ip_time = input_times_list[end_word_index].split(time_delimiter)[1].strip()\n",
    "                    # print(end_ip_time)\n",
    "\n",
    "                    fp.write(start_ip_time+time_delimiter+end_ip_time+time_delimiter+sentence+'\\n')\n",
    "    except:\n",
    "        print('Exception in extracted_text_to_output()')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "separate-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_output_text(input_file, output_file, output_text_file, num_sentences):\n",
    "\n",
    "    try:\n",
    "        time_delimiter = '-->'\n",
    "\n",
    "        input_text_list, input_times_list = subtitle_to_textblob(input_file)\n",
    "\n",
    "        input_text = ' '.join(input_text_list)\n",
    "        # print(input_text)\n",
    "        output_text = ''\n",
    "        \n",
    "        with open(output_text_file, 'r') as fp:\n",
    "            output_text = fp.read()\n",
    "            \n",
    "        extracted_text_to_output(input_text, output_text, output_file, input_times_list, time_delimiter)\n",
    "        print('Output with timestamps written to ', output_file)\n",
    "            \n",
    "    except:\n",
    "        print('Exception in extract()')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-script",
   "metadata": {},
   "source": [
    "### Example to run 'extract_from_output_text()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dominant-ecuador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with timestamps written to  data/podcast__transcription_test_op_15.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_20.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_25.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_30.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_35.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_40.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_45.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_50.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_60.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_70.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_80.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_90.txt\n",
      "Output with timestamps written to  data/podcast__transcription_test_op_100.txt\n"
     ]
    }
   ],
   "source": [
    "num_sentences_list = [15,20,25,30,35,40,45,50,60,70,80,90,100]\n",
    "input_file = 'data/podcast__transcription_test.srt'\n",
    "\n",
    "for num_sentences in num_sentences_list:\n",
    "    output_text_file = input_file.split('.')[0] + '_optext_' + str(num_sentences) + '.txt'\n",
    "    output_file = input_file.split('.')[0] + '_op_' + str(num_sentences) + '.txt'\n",
    "    \n",
    "    extract_from_output_text(input_file, output_file, output_text_file, num_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-maryland",
   "metadata": {},
   "source": [
    "## Extract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unlike-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_srt(input_file, output_file, num_sentences):\n",
    "\n",
    "    try:\n",
    "        time_delimiter = '-->'\n",
    "\n",
    "        input_text_list, input_times_list = subtitle_to_textblob(input_file)\n",
    "\n",
    "        input_text = ' '.join(input_text_list)\n",
    "\n",
    "        \n",
    "\n",
    "        output_text = extractive_summarization(input_text, num_sentences, True)\n",
    "        \n",
    "        with open(output_file, 'w') as fp:\n",
    "            fp.write(output_text)\n",
    "        \n",
    "        extracted_text_to_output(input_text, output_text, output_file, input_times_list, time_delimiter)\n",
    "    except:\n",
    "        print('Exception in extract()')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences_list = [10,15,20,25,30,35,40,45,50,60,70,80,90,100]\n",
    "\n",
    "ip_file = 'data/podcast__transcription_test.srt'\n",
    "\n",
    "for num_sentences in num_sentences_list:\n",
    "    \n",
    "    op_file = ip_file.split('.')[0] + '_op_' + str(num_sentences) + '.txt'\n",
    "    \n",
    "    # print('START ---', op_file)\n",
    "    extract_from_srt(ip_file, op_file, num_sentences)\n",
    "    # print('END ---', op_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-requirement",
   "metadata": {},
   "source": [
    "### Sample to run single iteration over num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "italic-muscle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nip_file = 'data/AE_Shopify Walkthrough 1.srt'\\nop_file = 'data/AE_Shopify Walkthrough 1_op.txt'\\nnum_sentences = 10\\n\\nextract(ip_file, op_file, num_sentences)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ip_file = 'data/AE_Shopify Walkthrough 1.srt'\n",
    "op_file = 'data/AE_Shopify Walkthrough 1_op.txt'\n",
    "num_sentences = 10\n",
    "\n",
    "extract_from_srt(ip_file, op_file, num_sentences)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-proof",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "podcast",
   "language": "python",
   "name": "podcast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
