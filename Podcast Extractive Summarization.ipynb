{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "shaped-treasure",
   "metadata": {},
   "source": [
    "# EXTRACTIVE SUMMARIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-formula",
   "metadata": {},
   "source": [
    "#### Dependencies\n",
    "\n",
    "* To avoid dependency issues, install the following versions\n",
    "\n",
    "Python = 3.6.9 <br>\n",
    "torch==1.7.0 <br> \n",
    "spacy==2.3.1 <br>\n",
    "bert-extractive-summarizer <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "progressive-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "import traceback \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-nation",
   "metadata": {},
   "source": [
    "### Parsing SRT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hearing-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtitle_to_textblob(subtitle_file):\n",
    "\n",
    "    input_text_list = list()\n",
    "    input_times_list = list()\n",
    "\n",
    "    count = 0\n",
    "    with open(subtitle_file, 'r') as fp:\n",
    "        input_lines = fp.readlines()\n",
    "        for line in input_lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            # print('Count ', count)\n",
    "            if (line):\n",
    "                # Process line numbers\n",
    "                if (count == 0):\n",
    "                    count += 1\n",
    "                elif (count == 1):\n",
    "                    input_times_list.append(line)\n",
    "                    count += 1\n",
    "                elif (count == 2):\n",
    "                    input_text_list.append(line)\n",
    "                    count = 0\n",
    "    return input_text_list, input_times_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "peripheral-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summarization(input_text, num_sentences, debug=False):\n",
    "\n",
    "    model = Summarizer()\n",
    "    output_text = model(input_text, num_sentences=num_sentences-1)\n",
    "    \n",
    "    if (debug):        \n",
    "        print('----------------- TOP',str(num_sentences),'SENTENCES -----------------')\n",
    "        print(output_text)\n",
    "        print('----------------------------------------------------')\n",
    "        \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "joined-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted_text_to_output(input_text, output_text, output_file, input_times_list, time_delimiter):\n",
    "    \n",
    "    try:\n",
    "        extracted_sentences = re.split(r'[.!?\\n]\\s*',output_text.strip())\n",
    "        print('Size of times list: ', len(input_times_list), ':Num Sentences = ', len(extracted_sentences))\n",
    "\n",
    "        count = 0\n",
    "        with open(output_file, 'w+') as fp:\n",
    "\n",
    "            for sentence in extracted_sentences:\n",
    "\n",
    "                sentence = sentence.strip()\n",
    "\n",
    "                if (sentence):\n",
    "\n",
    "                    # print(sentence)\n",
    "                    search_list = list(sentence.split())\n",
    "\n",
    "                    end_char_index = input_text.find(sentence)\n",
    "                    start_word_index = len(input_text[:end_char_index].split())\n",
    "                    end_word_index = start_word_index + len(search_list)-1\n",
    "                    \n",
    "                    # print(end_char_index, start_word_index, end_word_index)\n",
    "                    \n",
    "                    # print(start_word_index, end_word_index)\n",
    "\n",
    "                    start_ip_time = input_times_list[start_word_index].split(time_delimiter)[0].strip()\n",
    "                    # print(start_ip_time)\n",
    "\n",
    "                    end_ip_time = input_times_list[end_word_index].split(time_delimiter)[1].strip()\n",
    "                    # print(end_ip_time)\n",
    "\n",
    "                    fp.write(start_ip_time+time_delimiter+end_ip_time+time_delimiter+sentence+'\\n')\n",
    "                    count += 1\n",
    "                    print(count, end=' ')\n",
    "    except:\n",
    "        print('Exception in extracted_text_to_output()')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-mainland",
   "metadata": {},
   "source": [
    "## Run if output_text from extractive summarization is already available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rough-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_output_text(input_file, output_file, output_text_file, num_sentences):\n",
    "\n",
    "    try:\n",
    "        time_delimiter = '-->'\n",
    "\n",
    "        input_text_list, input_times_list = subtitle_to_textblob(input_file)\n",
    "\n",
    "        input_text = ' '.join(input_text_list)\n",
    "        # print(input_text)\n",
    "        output_text = ''\n",
    "        \n",
    "        with open(output_text_file, 'r') as fp:\n",
    "            output_text = fp.read()\n",
    "            \n",
    "        extracted_text_to_output(input_text, output_text, output_file, input_times_list, time_delimiter)\n",
    "        print('Output with timestamps written to ', output_file)\n",
    "            \n",
    "    except:\n",
    "        print('Exception in extract()')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-person",
   "metadata": {},
   "source": [
    "### Example to run 'extract_from_output_text()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unauthorized-myanmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in extract()\n",
      "Size of times list:  17425 :Num Sentences =  11\n",
      "1 2 3 4 5 6 7 8 9 10 Output with timestamps written to  data/podcast__transcription_test_op_10.txt\n",
      "Size of times list:  17425 :Num Sentences =  16\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Output with timestamps written to  data/podcast__transcription_test_op_15.txt\n",
      "Size of times list:  17425 :Num Sentences =  24\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Output with timestamps written to  data/podcast__transcription_test_op_20.txt\n",
      "Size of times list:  17425 :Num Sentences =  27\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Output with timestamps written to  data/podcast__transcription_test_op_25.txt\n",
      "Size of times list:  17425 :Num Sentences =  33\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Output with timestamps written to  data/podcast__transcription_test_op_30.txt\n",
      "Size of times list:  17425 :Num Sentences =  37\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 Output with timestamps written to  data/podcast__transcription_test_op_35.txt\n",
      "Size of times list:  17425 :Num Sentences =  42\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 Output with timestamps written to  data/podcast__transcription_test_op_40.txt\n",
      "Size of times list:  17425 :Num Sentences =  48\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-407fc54bafa3>\", line 12, in extract_from_output_text\n",
      "    with open(output_text_file, 'r') as fp:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/podcast__transcription_test_optext_5.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45 46 47 Output with timestamps written to  data/podcast__transcription_test_op_45.txt\n",
      "Size of times list:  17425 :Num Sentences =  51\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Output with timestamps written to  data/podcast__transcription_test_op_50.txt\n",
      "Size of times list:  17425 :Num Sentences =  61\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 Output with timestamps written to  data/podcast__transcription_test_op_60.txt\n",
      "Size of times list:  17425 :Num Sentences =  71\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Output with timestamps written to  data/podcast__transcription_test_op_70.txt\n",
      "Size of times list:  17425 :Num Sentences =  83\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 Output with timestamps written to  data/podcast__transcription_test_op_80.txt\n",
      "Size of times list:  17425 :Num Sentences =  90\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 Output with timestamps written to  data/podcast__transcription_test_op_90.txt\n",
      "Size of times list:  17425 :Num Sentences =  102\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 Output with timestamps written to  data/podcast__transcription_test_op_100.txt\n"
     ]
    }
   ],
   "source": [
    "num_sentences_list = [5,10,15,20,25,30,35,40,45,50,60,70,80,90,100]\n",
    "input_file = 'data/podcast__transcription_test.srt'\n",
    "\n",
    "for num_sentences in num_sentences_list:\n",
    "    output_text_file = input_file.split('.')[0] + '_optext_' + str(num_sentences) + '.txt'\n",
    "    output_file = input_file.split('.')[0] + '_op_' + str(num_sentences) + '.txt'\n",
    "    \n",
    "    extract_from_output_text(input_file, output_file, output_text_file, num_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-budget",
   "metadata": {},
   "source": [
    "## Run Extractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_srt(input_file, output_file, num_sentences):\n",
    "\n",
    "    try:\n",
    "        time_delimiter = '-->'\n",
    "\n",
    "        input_text_list, input_times_list = subtitle_to_textblob(input_file)\n",
    "\n",
    "        input_text = ' '.join(input_text_list)\n",
    "\n",
    "        \n",
    "\n",
    "        output_text = extractive_summarization(input_text, num_sentences, True)\n",
    "        \n",
    "        with open(output_file, 'w') as fp:\n",
    "            fp.write(output_text)\n",
    "        \n",
    "        extracted_text_to_output(input_text, output_text, output_file, input_times_list, time_delimiter)\n",
    "    except:\n",
    "        print('Exception in extract()')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-border",
   "metadata": {},
   "source": [
    "### Example to run 'extract_from_srt()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences_list = [10,15,20,25,30,35,40,45,50,60,70,80,90,100]\n",
    "\n",
    "ip_file = 'data/podcast__transcription_test.srt'\n",
    "\n",
    "for num_sentences in num_sentences_list:\n",
    "    \n",
    "    op_file = ip_file.split('.')[0] + '_op_' + str(num_sentences) + '.txt'\n",
    "    \n",
    "    # print('START ---', op_file)\n",
    "    extract_from_srt(ip_file, op_file, num_sentences)\n",
    "    # print('END ---', op_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-details",
   "metadata": {},
   "source": [
    "### Sample to run single iteration over num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ip_file = 'data/AE_Shopify Walkthrough 1.srt'\n",
    "op_file = 'data/AE_Shopify Walkthrough 1_op.txt'\n",
    "num_sentences = 10\n",
    "\n",
    "extract_from_srt(ip_file, op_file, num_sentences)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nervous-sudan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list1 length:  26\n",
      "list2 length:  23\n",
      "Common elements 9\n"
     ]
    }
   ],
   "source": [
    "ip_file = 'data/podcast__transcription_test.srt'\n",
    "\n",
    "op_1 = 25\n",
    "op_2 = 20\n",
    "\n",
    "delimiter = '-->'\n",
    "\n",
    "output_file1 = input_file.split('.')[0] + '_op_' + str(op_1) + '.txt'\n",
    "output_file2= input_file.split('.')[0] + '_op_' + str(op_2) + '.txt'\n",
    "\n",
    "list1 = list()\n",
    "with open(output_file1, 'r') as fp:\n",
    "    input_lines = fp.readlines()\n",
    "    for line in input_lines:\n",
    "        times = line.split(delimiter)\n",
    "        list1.append(times[0]+' '+times[1])\n",
    "        \n",
    "list2 = list()\n",
    "with open(output_file2, 'r') as fp:\n",
    "    input_lines = fp.readlines()\n",
    "    for line in input_lines:\n",
    "        times = line.split(delimiter)\n",
    "        list2.append(times[0]+' '+times[1])\n",
    "        \n",
    "print('list1 length: ', len(list1))\n",
    "print('list2 length: ', len(list2))\n",
    "print('Common elements', len(set(list1).intersection(list2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_list, input_times_list = subtitle_to_textblob('data/podcast__transcription_test.srt')\n",
    "input_text = ' '.join(input_text_list)\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-creativity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "podcast",
   "language": "python",
   "name": "podcast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
